{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e9b639",
   "metadata": {},
   "source": [
    "\n",
    "# Neural Network Training — Keras (TensorFlow) Demo\n",
    "\n",
    "This notebook mirrors the NumPy/PyTorch demos using **Keras** for a compact, high-level API.\n",
    "\n",
    "**Covers:**\n",
    "- Synthetic 2D dataset with train/val split and standardization\n",
    "- MLP: 2 → H → 1 with ReLU + sigmoid, `binary_crossentropy`\n",
    "- Training with `model.fit`, validation metrics, and plots\n",
    "- Experiments: learning rate, L2 regularization (`kernel_regularizer`), dropout\n",
    "- Seed sensitivity (mean±std val accuracy)\n",
    "- Label-noise robustness\n",
    "- Gradient sanity check with `tf.GradientTape`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "device = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0b270",
   "metadata": {},
   "source": [
    "## 1) Data: 2D blobs + standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_blobs(n_per_class=400, spread=1.1, offset=2.0, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mean0 = np.array([-offset, 0.0])\n",
    "    mean1 = np.array([ offset, 0.0])\n",
    "    cov = spread * np.eye(2)\n",
    "    X0 = rng.multivariate_normal(mean0, cov, size=n_per_class)\n",
    "    X1 = rng.multivariate_normal(mean1, cov, size=n_per_class)\n",
    "    X = np.vstack([X0, X1]).astype(np.float32)\n",
    "    y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)]).astype(np.float32).reshape(-1,1)\n",
    "    idx = rng.permutation(len(X))\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def standardize(X):\n",
    "    m = X.mean(axis=0, keepdims=True)\n",
    "    s = X.std(axis=0, keepdims=True) + 1e-8\n",
    "    return (X - m)/s, m, s\n",
    "\n",
    "X, y = make_blobs(n_per_class=400, spread=1.1, offset=2.0, seed=42)\n",
    "X, X_mean, X_std = standardize(X)\n",
    "\n",
    "n = len(X)\n",
    "split = int(0.8*n)\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_val, y_val = X[split:], y[split:]\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape, \"| Val:\", X_val.shape, y_val.shape)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train.flatten(), s=10, alpha=0.6)\n",
    "plt.title(\"Training Data (standardized)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbde8ec",
   "metadata": {},
   "source": [
    "## 2) Keras Model: 2 → H → 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hidden_dim=16, lr=0.05, l2=0.0, dropout_p=0.0, seed=42):\n",
    "    set_seed(seed)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(2,)),\n",
    "        layers.Dense(hidden_dim, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(l2) if l2>0 else None),\n",
    "        layers.Dropout(dropout_p) if dropout_p>0 else layers.Lambda(lambda x: x),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    opt = keras.optimizers.SGD(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def train_model(hidden_dim=16, lr=0.05, l2=0.0, dropout_p=0.0,\n",
    "                epochs=200, batch_size=64, seed=42, verbose=0):\n",
    "    model = build_model(hidden_dim, lr, l2, dropout_p, seed)\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                     epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    return model, hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6f5b2",
   "metadata": {},
   "source": [
    "## 3) Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, hist = train_model(hidden_dim=16, lr=0.05, l2=0.0, dropout_p=0.0,\n",
    "                          epochs=200, batch_size=64, seed=42, verbose=0)\n",
    "val_acc = model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "print(\"Validation accuracy (baseline):\", round(val_acc, 4))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Loss Curves (Baseline)\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e4595",
   "metadata": {},
   "source": [
    "## 4) Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe9b2f",
   "metadata": {},
   "source": [
    "### A) Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fba7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, h1 = train_model(lr=0.5, epochs=100, verbose=0, seed=100)\n",
    "_, h2 = train_model(lr=0.05, epochs=100, verbose=0, seed=100)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(h1.history[\"val_loss\"], label=\"val (lr=0.5)\")\n",
    "plt.plot(h2.history[\"val_loss\"], label=\"val (lr=0.05)\")\n",
    "plt.title(\"Learning Rate Comparison\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963ec23",
   "metadata": {},
   "source": [
    "### B) L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33960b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, h3 = train_model(lr=0.05, l2=1e-3, epochs=200, verbose=0, seed=123)\n",
    "_, h4 = train_model(lr=0.05, l2=1e-2, epochs=200, verbose=0, seed=123)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(h3.history[\"val_loss\"], label=\"val (l2=1e-3)\")\n",
    "plt.plot(h4.history[\"val_loss\"], label=\"val (l2=1e-2)\")\n",
    "plt.title(\"L2 Regularization Comparison\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da818a9e",
   "metadata": {},
   "source": [
    "### C) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952eafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, h5 = train_model(lr=0.05, dropout_p=0.2, epochs=200, verbose=0, seed=222)\n",
    "_, h6 = train_model(lr=0.05, dropout_p=0.5, epochs=200, verbose=0, seed=222)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(h5.history[\"val_loss\"], label=\"val (dropout=0.2)\")\n",
    "plt.plot(h6.history[\"val_loss\"], label=\"val (dropout=0.5)\")\n",
    "plt.title(\"Dropout Comparison\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d17d1",
   "metadata": {},
   "source": [
    "### D) Seed Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_sweep(n_runs=10, base_seed=0, **kwargs):\n",
    "    accs = []\n",
    "    for i in range(n_runs):\n",
    "        seed = base_seed + i*17\n",
    "        m, h = train_model(seed=seed, verbose=0, **kwargs)\n",
    "        acc = m.evaluate(X_val, y_val, verbose=0)[1]\n",
    "        accs.append(acc)\n",
    "    return np.array(accs)\n",
    "\n",
    "accs = seed_sweep(n_runs=10, base_seed=0, hidden_dim=16, lr=0.05, l2=0.0, dropout_p=0.0,\n",
    "                  epochs=200, batch_size=64)\n",
    "print(\"Seed sweep (n=10) — val acc mean±std:\", round(accs.mean(),4), \"±\", round(accs.std(),4))\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(accs, marker='o'); plt.ylim(0,1); plt.title(\"Val Accuracy across Seeds\")\n",
    "plt.xlabel(\"Run\"); plt.ylabel(\"Val Acc\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da329c41",
   "metadata": {},
   "source": [
    "### E) Label-Noise Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flip_labels(y, noise_rate=0.1, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y2 = y.copy()\n",
    "    m = len(y2)\n",
    "    idx = rng.choice(m, size=int(noise_rate*m), replace=False)\n",
    "    y2[idx] = 1 - y2[idx]\n",
    "    return y2\n",
    "\n",
    "for noise in [0.0, 0.1, 0.2]:\n",
    "    y_tr_noisy = flip_labels(y_train, noise_rate=noise, seed=999)\n",
    "    print(f\"\\nNoise rate = {noise:.1f}\")\n",
    "    # No L2\n",
    "    m1 = build_model(lr=0.05, l2=0.0, seed=321)\n",
    "    m1.fit(X_train, y_tr_noisy, validation_data=(X_val, y_val), epochs=200, batch_size=64, verbose=0)\n",
    "    acc1 = m1.evaluate(X_val, y_val, verbose=0)[1]\n",
    "    # With L2\n",
    "    m2 = build_model(lr=0.05, l2=1e-3, seed=321)\n",
    "    m2.fit(X_train, y_tr_noisy, validation_data=(X_val, y_val), epochs=200, batch_size=64, verbose=0)\n",
    "    acc2 = m2.evaluate(X_val, y_val, verbose=0)[1]\n",
    "    print(f\"Val acc (no L2): {acc1:.3f} | Val acc (L2=1e-3): {acc2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92ae0d",
   "metadata": {},
   "source": [
    "## 5) Gradient Sanity Check with `tf.GradientTape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small batch\n",
    "Xb = X_train[:5]\n",
    "yb = y_train[:5]\n",
    "\n",
    "model_gc = build_model(hidden_dim=5, lr=0.05, l2=1e-3, dropout_p=0.0, seed=7)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    preds = model_gc(Xb, training=True)\n",
    "    loss = keras.losses.binary_crossentropy(yb, preds)\n",
    "    loss = tf.reduce_mean(loss) + sum(model_gc.losses)  # include L2 if any\n",
    "\n",
    "grads = tape.gradient(loss, model_gc.trainable_variables)\n",
    "for var, g in zip(model_gc.trainable_variables, grads):\n",
    "    print(var.name, \"grad shape:\", None if g is None else g.shape)\n",
    "print(\"Gradient sanity check complete.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
