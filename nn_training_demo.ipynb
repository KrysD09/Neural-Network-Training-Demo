{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1453483c",
   "metadata": {},
   "source": [
    "# Neural Network Training â€” NumPy From Scratch\n",
    "\n",
    "A minimal 2-layer neural network with manual forward/backprop and gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "def make_data(n=400):\n",
    "    X = np.random.randn(n, 2)\n",
    "    y = (X[:,0]*X[:,1] > 0).astype(int).reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data()\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize weights\n",
    "n_input, n_hidden, n_output = 2, 5, 1\n",
    "W1 = 0.01 * np.random.randn(n_input, n_hidden)\n",
    "b1 = np.zeros((1, n_hidden))\n",
    "W2 = 0.01 * np.random.randn(n_hidden, n_output)\n",
    "b2 = np.zeros((1, n_output))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(a):\n",
    "    return a * (1 - a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop (basic)\n",
    "lr = 0.1\n",
    "for epoch in range(2000):\n",
    "    # Forward\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.maximum(0, z1)  # ReLU\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    y_hat = sigmoid(z2)\n",
    "    loss = -(y*np.log(y_hat+1e-8) + (1-y)*np.log(1-y_hat+1e-8)).mean()\n",
    "    \n",
    "    # Backprop\n",
    "    dz2 = y_hat - y\n",
    "    dW2 = a1.T.dot(dz2) / len(X)\n",
    "    db2 = dz2.mean(axis=0, keepdims=True)\n",
    "    da1 = dz2.dot(W2.T)\n",
    "    dz1 = da1 * (z1 > 0)\n",
    "    dW1 = X.T.dot(dz1) / len(X)\n",
    "    db1 = dz1.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    # Update\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, loss {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient check example\n",
    "eps = 1e-5\n",
    "ix = (0,0)\n",
    "oldval = W1[ix]\n",
    "W1[ix] = oldval + eps\n",
    "z1 = X.dot(W1) + b1\n",
    "a1 = np.maximum(0, z1)\n",
    "z2 = a1.dot(W2) + b2\n",
    "y_hat = sigmoid(z2)\n",
    "loss_plus = -(y*np.log(y_hat+1e-8)+(1-y)*np.log(1-y_hat+1e-8)).mean()\n",
    "\n",
    "W1[ix] = oldval - eps\n",
    "z1 = X.dot(W1) + b1\n",
    "a1 = np.maximum(0, z1)\n",
    "z2 = a1.dot(W2) + b2\n",
    "y_hat = sigmoid(z2)\n",
    "loss_minus = -(y*np.log(y_hat+1e-8)+(1-y)*np.log(1-y_hat+1e-8)).mean()\n",
    "\n",
    "W1[ix] = oldval\n",
    "num_grad = (loss_plus - loss_minus)/(2*eps)\n",
    "print(\"Numerical grad:\", num_grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
